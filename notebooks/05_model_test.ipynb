{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab8d86c-4606-4703-bc77-9c8e5179f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf5be61-7238-4051-9e2f-d96a51b63766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b012c52b-9fa5-4732-9ae4-ccb166ee63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "svc = SVC(kernel='sigmoid',gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state = 32)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=32)\n",
    "etc = ExtraTreesClassifier(n_estimators = 50, random_state = 32)\n",
    "gbdt = GradientBoostingClassifier(n_estimators = 50,random_state = 32)\n",
    "xgb = XGBClassifier(n_estimators=50, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a31c410-39e7-4c6f-b544-d269e488ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls = {\n",
    "    \"mnb\":mnb,\n",
    "    \"lr\":lr,\n",
    "    \"svc\":svc,\n",
    "    \"knc\":knc,\n",
    "    \"dtc\":dtc,\n",
    "    \"abc\":abc,\n",
    "    \"bc\":bc,\n",
    "    \"etc\":etc,\n",
    "    \"gbdt\":gbdt,\n",
    "    \"xgb\":xgb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07a8110-bd05-4898-8198-f9794c8aa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_mdl(mdl, X_train, y_train, X_test, y_test):\n",
    "#     mdl.fit(X_train, y_train)\n",
    "#     print(\"model trained\")\n",
    "#     y_preds = mdl.predict(X_test)\n",
    "#     # return accuracy_score(y_preds, y_test), precision_score(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a68c632a-4b63-4ac1-abfe-0ff014e80e22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_mdl(lr,\u001b[43mX_train\u001b[49m, y_train, X_test, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# predict_mdl(lr,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3207e55-d57f-49ed-bfed-e82b386aad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3940b25-4e94-4e7f-8df8-693da08cfbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/spam_new.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a05f35-4593-4a74-8dcd-440076bb4099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3214731-d302-47c5-9620-435f97e8f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'v1':'status', 'v2':'sms'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742aed14-99cb-4e70-b378-404d52e233ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], axis= 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2775ea-6ff5-4697-886a-526e7f114306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'] = df['status'].map({\"ham\":0, \"spam\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e403593c-7cf7-47ca-8f57-4fca6fec0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from word_process import WordProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f51db8e-cc0d-4c4b-ba57-5c557029c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wp = WordProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3df03f-c617-47d4-9b8d-11a631d798b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56ab3b4-842f-479f-ad0d-7fb3e56d90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aaa82ed0-4797-4ffb-b9df-373e5fdc8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer()\n",
    "# cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64cb200c-eed7-4da2-b18c-6566682faa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59c6513c-6d05-47a6-96a4-739270583928",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "            \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "            \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0855878-583e-41d4-a3f4-5fb823ed1f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=3000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=3000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=3000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = TfidfVectorizer(max_features=3000)\n",
    "cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c8fb641-004a-42af-969b-d325856e6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['processed'] = df['sms'].apply(wp.process_sent2sent)\n",
    "df['processed'] = df['sms'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c4ceb11-8adc-4583-a65e-57c0266c1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['processed'],\n",
    "                                                    df['status'],test_size=0.2, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a46191c9-81b8-4fb2-9e33-6a30294c6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ff6be72-7481-4b4c-a893-bcbdaffa1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c98233a6-c234-48e7-aeeb-e8e529574048",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cv = np.expand_dims(np.array(y_train),axis=-1)\n",
    "y_test_cv = np.expand_dims(np.array(y_test),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aed2c05d-65e3-48e1-a8cf-4295a01e3a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 3000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a99b605f-f202-4b32-876d-6ca50703b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704035874439462\n",
      "[[969   0]\n",
      " [ 33 113]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "mnb.fit(X_train_cv,y_train)\n",
    "y_pred2 = mnb.predict(X_test_cv)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ae22439-3371-4197-9519-9903833fe9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9704035874439462, 0.773972602739726)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr.fit(X_train_cv,y_train)\n",
    "predict_mdl(mnb,X_train_cv, y_train, X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad946cd8-adec-4791-8473-c9057c62c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_mdl(mdl, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains model and returns (accuracy,precision)\"\"\"\n",
    "    mdl.fit(X_train, y_train)\n",
    "    y_preds = mdl.predict(X_test)\n",
    "\n",
    "    # print('hi')\n",
    "    print(accuracy_score(y_test,y_preds))\n",
    "    print(confusion_matrix(y_test,y_preds))\n",
    "    print(precision_score(y_test,y_preds))\n",
    "\n",
    "    return accuracy_score(y_preds, y_test), precision_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54a9a7d4-3c8b-49da-b03b-f565db3272a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_mdl(lr,X_train_cv, y_train, X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18bc345d-2bff-4676-9382-d9bf24fa4c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---===------===------===------===------===---\n",
      "mnb\n",
      "0.9704035874439462\n",
      "[[969   0]\n",
      " [ 33 113]]\n",
      "1.0\n",
      "0.9704035874439462 1.0\n",
      "\n",
      "---===------===------===------===------===---\n",
      "lr\n",
      "0.9659192825112107\n",
      "[[968   1]\n",
      " [ 37 109]]\n",
      "0.990909090909091\n",
      "0.9659192825112107 0.990909090909091\n",
      "\n",
      "---===------===------===------===------===---\n",
      "svc\n",
      "0.9802690582959641\n",
      "[[967   2]\n",
      " [ 20 126]]\n",
      "0.984375\n",
      "0.9802690582959641 0.984375\n",
      "\n",
      "---===------===------===------===------===---\n",
      "knc\n",
      "0.9139013452914798\n",
      "[[969   0]\n",
      " [ 96  50]]\n",
      "1.0\n",
      "0.9139013452914798 1.0\n",
      "\n",
      "---===------===------===------===------===---\n",
      "dtc\n",
      "0.9354260089686098\n",
      "[[952  17]\n",
      " [ 55  91]]\n",
      "0.8425925925925926\n",
      "0.9354260089686098 0.8425925925925926\n",
      "\n",
      "---===------===------===------===------===---\n",
      "abc\n",
      "0.9730941704035875\n",
      "[[967   2]\n",
      " [ 28 118]]\n",
      "0.9833333333333333\n",
      "0.9730941704035875 0.9833333333333333\n",
      "\n",
      "---===------===------===------===------===---\n",
      "bc\n",
      "0.9659192825112107\n",
      "[[956  13]\n",
      " [ 25 121]]\n",
      "0.9029850746268657\n",
      "0.9659192825112107 0.9029850746268657\n",
      "\n",
      "---===------===------===------===------===---\n",
      "etc\n",
      "0.9820627802690582\n",
      "[[967   2]\n",
      " [ 18 128]]\n",
      "0.9846153846153847\n",
      "0.9820627802690582 0.9846153846153847\n",
      "\n",
      "---===------===------===------===------===---\n",
      "gbdt\n",
      "0.9497757847533632\n",
      "[[965   4]\n",
      " [ 52  94]]\n",
      "0.9591836734693877\n",
      "0.9497757847533632 0.9591836734693877\n",
      "\n",
      "---===------===------===------===------===---\n",
      "xgb\n",
      "0.967713004484305\n",
      "[[965   4]\n",
      " [ 32 114]]\n",
      "0.9661016949152542\n",
      "0.967713004484305 0.9661016949152542\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "for name, mdl in mdls.items():\n",
    "    print()\n",
    "    print(\"---===---\"*5)\n",
    "    print(name)\n",
    "    (acc, pre) = predict_mdl(mdl, X_train_cv, y_train, X_test_cv, y_test)\n",
    "    print(acc, pre)\n",
    "    names.append(name)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67e1b5fb-7dd9-4993-b2dd-4ee974573897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['lr', 'svc', 'knc', 'mnb', 'dtc', 'abc', 'bc', 'etc', 'gbdt', 'xgb'],\n",
       " [0.9659192825112107,\n",
       "  0.9802690582959641,\n",
       "  0.9139013452914798,\n",
       "  0.9704035874439462,\n",
       "  0.9354260089686098,\n",
       "  0.9730941704035875,\n",
       "  0.9659192825112107,\n",
       "  0.9820627802690582,\n",
       "  0.9497757847533632,\n",
       "  0.967713004484305],\n",
       " [0.7465753424657534,\n",
       "  0.863013698630137,\n",
       "  0.3424657534246575,\n",
       "  0.773972602739726,\n",
       "  0.6232876712328768,\n",
       "  0.8082191780821918,\n",
       "  0.8287671232876712,\n",
       "  0.8767123287671232,\n",
       "  0.6438356164383562,\n",
       "  0.7808219178082192])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names,accuracies, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2241eaaa-dc78-4538-9d80-7ad796758375",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = pd.DataFrame({\"models\":names,\"accuracy\":accuracies,\"precision\":precisions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "aceca8bf-a769-4262-b8af-880e5a5d7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec['yt_tfidf_m3k_acc'] = accuracies\n",
    "# rec['yt_tfidf_m3k_prec'] = precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "442dab99-3792-4b0a-8fad-849d23ecd08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.746575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.863014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knc</td>\n",
       "      <td>0.913901</td>\n",
       "      <td>0.342466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnb</td>\n",
       "      <td>0.970404</td>\n",
       "      <td>0.773973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dtc</td>\n",
       "      <td>0.935426</td>\n",
       "      <td>0.623288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abc</td>\n",
       "      <td>0.973094</td>\n",
       "      <td>0.808219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bc</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.828767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>etc</td>\n",
       "      <td>0.982063</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gbdt</td>\n",
       "      <td>0.949776</td>\n",
       "      <td>0.643836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.967713</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  models  accuracy  precision\n",
       "0     lr  0.965919   0.746575\n",
       "1    svc  0.980269   0.863014\n",
       "2    knc  0.913901   0.342466\n",
       "3    mnb  0.970404   0.773973\n",
       "4    dtc  0.935426   0.623288\n",
       "5    abc  0.973094   0.808219\n",
       "6     bc  0.965919   0.828767\n",
       "7    etc  0.982063   0.876712\n",
       "8   gbdt  0.949776   0.643836\n",
       "9    xgb  0.967713   0.780822"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rec.sort_values(by=['yt_tfidf_m3k_prec','yt_tfidf_m3k_acc'],ascending=[False,False])\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b56d4d91-14f6-40dd-9de8-e6e2f1c4108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec.to_csv(\"spam_lemm.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1f5866e-2463-42c5-a8dc-5d43464ad667",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fc6d47ba-2a2f-4377-a960-3e114cb02f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(cv.transform([wp.process_sent2sent(text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "904e9209-0582-48ff-b0e3-5c42c33da8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = df[df['status']==1].iloc[98]['sms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6376dd43-d896-4439-bd6c-700f5dd3759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 70\n",
    "text = X_test.iloc[ind]\n",
    "y_test.iloc[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3345a00-4222-4466-9285-bdf9f5c58597",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/suraj/nltk_data'\n    - '/Applications/anaconda3/envs/aimlEnv/nltk_data'\n    - '/Applications/anaconda3/envs/aimlEnv/share/nltk_data'\n    - '/Applications/anaconda3/envs/aimlEnv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransform_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m, in \u001b[0;36mtransform_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_text\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m----> 3\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m text:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/aimlEnv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/aimlEnv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/aimlEnv/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/aimlEnv/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/aimlEnv/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/suraj/nltk_data'\n    - '/Applications/anaconda3/envs/aimlEnv/nltk_data'\n    - '/Applications/anaconda3/envs/aimlEnv/share/nltk_data'\n    - '/Applications/anaconda3/envs/aimlEnv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "transform_text(df['sms'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9e83d1d3-203a-4e9d-a556-efae7589c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "class WordProcess1:\n",
    "    def __init__(self):\n",
    "        # self.lemm = WordNetLemmatizer()\n",
    "        self.lemm = PorterStemmer()\n",
    "        nltk.download(\"stopwords\")\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "    def get_wordnet_pos(self, pos):\n",
    "        if pos.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif pos.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif pos.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif pos.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    def process_sentence(self, sent):\n",
    "        # lowercasing\n",
    "        sen = sent.lower()\n",
    "\n",
    "        # tokenizing\n",
    "        tkns = wordpunct_tokenize(sen)\n",
    "\n",
    "        # removing stopwords and punctuations\n",
    "        stops = stopwords.words('english')\n",
    "        stops.extend([\"..\",\"...\",])\n",
    "        puncts = string.punctuation\n",
    "        clean = []\n",
    "        for word in tkns:\n",
    "            if word not in stops and word not in puncts:\n",
    "            # if len(word) > 1 and word not in stops and word not in puncts:\n",
    "                clean.append(word)\n",
    "\n",
    "        # word lemmatization\n",
    "        word_tags = nltk.pos_tag(clean)\n",
    "        word_lemm = []\n",
    "        for word,tag in word_tags:\n",
    "            # word_lemm.append(self.lemm.lemmatize(word,self.get_wordnet_pos(tag)))\n",
    "            word_lemm.append(self.lemm.stem(word))\n",
    "\n",
    "        sen = None\n",
    "        tkns = None\n",
    "        clean = None\n",
    "        word_tags = None\n",
    "\n",
    "        return word_lemm\n",
    "    \n",
    "    def process_sent2sent(self, sent):\n",
    "        return \" \".join(self.process_sentence(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1e6c19a3-37eb-4f38-92e3-ad6bbb32582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/suraj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "wp = WordProcess1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c8335501-9042-4070-94ab-598922557934",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.to_csv(\"spam_models_v2-1Feb-0128.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09281df6-773c-4108-ab63-6513131895e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "812d35ec-1bb2-4a9e-9606-6cc57886061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mnb, open(\"spam_mnb_model_1Feb_0202.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a148a785-311e-4004-80fa-00dfbefb1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv,open(\"spam_tfidf_vec_1Feb_0202.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926e08e-3ce7-458a-9443-a4a3cf9706d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
